# ðŸ“˜ Linear Regression

**Linear Regression** is one of the fundamental algorithms in supervised learning, used to model a continuous target variable as a linear combination of input features. It is simple, fast, interpretable, and forms the foundation for many advanced ML and statistical models.

It also introduces key ML concepts: optimization, cost minimization, gradients, biasâ€“variance, data diagnostics, regularization, and learning dynamics.

---

# **1. Introduction**
Linear Regression assumes:

- The relationship between predictors and the target is **linear**  
- Effects of each feature are **additive**  
- Coefficients represent the **strength and direction** of influence  

It is widely used for predictive modeling, statistical inference, and baseline modeling before applying complex algorithms.

---

# **2. Simple vs Multiple Linear Regression**
- **Simple Linear Regression**  
  - 1 independent variable  
  - Great for visual intuition  
- **Multiple Linear Regression**  
  - Multiple predictors influence the target  
  - Can capture richer patterns using interactions & polynomial features  

---

# **3. Mathematical Formulation**

```math
\hat{y} = \beta_0 + \beta_1 x_1 + \dots + \beta_n x_n
```

Where:  
- \( \beta_0 \) â†’ intercept  
- \( \beta_i \) â†’ slope/weight of feature \(x_i\)  
- \( \hat{y} \) â†’ predicted output  

Coefficients show how much the target changes per unit change in each feature, holding others constant.

---

# **4. Cost Function (RSS / MSE)**

**Residual Sum of Squares (RSS)**

```math
RSS = \sum (y - \hat{y})^2
```

**Mean Squared Error (MSE)**

```math
MSE = \frac{1}{m} \sum (y - \hat{y})^2
```

MSE penalizes large errors more strongly, making the model sensitive to outliers.

---

# **5. The Normal Equation**

The optimal parameters of Linear Regression can be computed analytically using:

```math
\beta = (X^T X)^{-1} X^T y
```

### When Normal Equation is ideal:
- Dataset is small (< 10,000 features)
- You want an exact closed-form solution
- Gradient descent complexity is unnecessary

### Limitations:
- Requires matrix inversion â†’ unstable with multicollinearity  
- Computationally expensive for high-dimensional data  

---

# **6. Computational Complexity**

### âœ” Normal Equation Complexity  
The dominant operation:

```math
(X^T X)^{-1}
```

Cost: **O(nÂ³)** where n = number of features  

This becomes slow or impossible when:

- n > 10,000  
- Matrix is singular or near-singular  

### âœ” Gradient Descent Complexity  
Each iteration cost:

**O(m Ã— n)**  
(m = samples, n = features)

Gradient descent scales better with high-dimensional data.

---

# **7. Gradient Descent (Core Optimization Method)**

Gradient Descent iteratively adjusts parameters to reduce the cost function.

### Update Rule:
```math
\beta_j := \beta_j - \alpha \cdot \frac{1}{m} \sum ( \hat{y} - y ) x_j
```

Where:
- \( \alpha \) â†’ learning rate  
- Good \( \alpha \) is critical for convergence  

GD introduces:
- Learning dynamics  
- Convergence interpretation  
- Optimization intuition  

---

# **8. Batch Gradient Descent**

### Definition:
Uses **all samples** to compute gradient in every update.

### Pros:
- Stable convergence  
- Smooth gradient steps  

### Cons:
- Very slow for large datasets  
- Memory-heavy  

Best for small â†’ medium datasets.

---

# **9. Stochastic Gradient Descent (SGD)**

### Definition:
Updates weights using **one sample at a time**.

### Pros:
- Extremely fast  
- Handles massive datasets  
- Avoids local minima by adding noise  

### Cons:
- Noisy updates  
- Requires careful learning rate tuning  
- Cost function fluctuates  

SGD is used in deep learning due to efficiency.

---

# **10. Mini-batch Gradient Descent**

### Definition:
Uses **small batches** (e.g., 32, 64, 128 samples) per update.

### Pros:
- Stable updates (less noise)  
- GPU-friendly  
- Faster convergence than batch GD  
- More controlled than SGD  

This is the **industry standard** for ML optimization.

---

# **11. Polynomial Regression**

Linear Regression can model curved relationships using polynomial features:

```math
y = \beta_0 + \beta_1 x + \beta_2 x^2 + ... + \beta_d x^d
```

### Benefits:
- Captures non-linear patterns  
- Still solved with linear regression (linear in parameters)

### Pitfalls:
- Overfitting at high degrees  
- Requires regularization (Ridge/Lasso)  
- Becomes sensitive to feature scaling  

---

# **12. Learning Curves**

Learning curves help diagnose:

- High bias  
- High variance  
- Need for more data  
- Underfitting/overfitting  

### A learning curve plots:
- **Training error** vs training size  
- **Validation error** vs training size  

### Interpretation:

| Pattern | Meaning | Fix |
|---------|---------|------|
| Both errors high | Underfitting | Add complexity, polynomial features |
| Large gap between errors | Overfitting | Add data, regularization |
| Plateau | Need more features | Feature engineering |

Learning curves are essential for deciding whether to collect more data or engineer better features.

---

# **13. Feature Engineering**

Linear models rely heavily on data quality.

### Important techniques:
- **Standardization / normalization**
- **Outlier removal**
- **Polynomial & interaction features**
- **Categorical encoding**
- **Domain-driven transformations**

---

# **14. Regularization Extensions**

### âœ” Ridge Regression (L2)
Smooth coefficient shrinkage

### âœ” Lasso Regression (L1)
Feature selection through sparsity

### âœ” Elastic Net
Balanced L1 + L2 solution

---

# **15. Evaluation Metrics**

- **RMSE** â€“ penalizes large errors  
- **MAE** â€“ robust to outliers  
- **RÂ² Score** â€“ variance explained  
- **Adjusted RÂ²** â€“ adjusts for number of predictors  

---

# **16. Residual Diagnostics**

Residuals help identify:

- Non-linearity  
- Outliers  
- Heteroscedasticity  
- Missing variables  

Key plots:
- Residual vs fitted  
- Q-Q plot  
- Histogram  
- Scale-location plot  

---

# **17. Assumptions of Linear Regression**

| Assumption | Meaning | Why It Matters | How to Check |
|-----------|---------|----------------|--------------|
| Linearity | Target is linear function of predictors | Prevents bias | Residuals plot |
| Independence | Errors are independent | Prevents inflated significance | Durbinâ€“Watson |
| Homoscedasticity | Constant variance of errors | Validates CI & p-values | BP test |
| Normality | Residuals normally distributed | Needed for inference | Q-Q plot |
| No multicollinearity | Predictors not correlated | Stable coefficients | VIF |

---

# **18. Multicollinearity & VIF**

High VIF â†’ unstable coefficients.

```math
VIF = \frac{1}{1 - R_j^2}
```

Fix using:

- Drop correlated variables  
- Combine features  
- Use PCA  
- Apply Ridge Regression  

---

# **19. When It Works Well**

- Relationships approx. linear  
- Interpretability is important  
- Data is small/medium sized  

---

# **20. When It Fails**

- Strongly non-linear patterns  
- Many outliers  
- High feature correlation  
- Complex interactions missing  

---

# **21. Real-World Applications**

- Pricing elasticity  
- Market forecasting  
- Telecom load prediction  
- Medical dosage prediction  
- Financial modeling  

---

# **22. Scikit-Learn Example**

```python
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)
preds = model.predict(X_test)
```

