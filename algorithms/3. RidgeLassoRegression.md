# üìò Ridge & Lasso Regression (Regularized Linear Models)

**Ridge** and **Lasso** are extensions of Linear Regression that add a **penalty on large coefficients** to reduce overfitting and improve generalization.

They are especially useful when:

- You have many correlated features  
- You want to control model complexity  
- You want more stable, robust models than plain OLS  

This section covers:

- Why we need regularization  
- Ridge Regression (L2)  
- Lasso Regression (L1)  
- Elastic Net (L1 + L2)  
- When to use which  
- Hyperparameter tuning and practical tips  
- Evaluation & interpretation  
- Scikit-Learn examples  

---

## 1. Why Regularization?

Plain Linear Regression (OLS) minimizes:

```text
MSE = (1 / m) * Œ£ (y - y_hat)^2
```

Problems in practice:

- Overfitting when you have many features relative to samples  
- Coefficients become unstable when features are correlated (multicollinearity)  
- Small changes in data can cause large changes in coefficients  

**Regularization** solves this by adding a penalty term to the loss that discourages large coefficients.

---

## 2. Ridge Regression (L2 Regularization)

Ridge adds an **L2 penalty** (square of the coefficients) to the loss.

### 2.1 Objective Function

```text
J_ridge = MSE + Œ± * Œ£ (Œ≤j^2)
```

- `MSE` ‚Äì mean squared error  
- `Œ≤j` ‚Äì model coefficients  
- `Œ±` (alpha) ‚Äì regularization strength (Œ± ‚â• 0)

Intuition:

- Penalizes large weights  
- Forces the model to spread influence across features  
- Helps stabilize coefficients when features are correlated  

### 2.2 Effect on Coefficients

- Coefficients are **shrunk towards zero**, but typically **not exactly zero**  
- All features remain in the model, but with reduced impact  
- Model becomes **less sensitive to noise**, more robust  

### 2.3 When Ridge Helps

- Many correlated features (multicollinearity)  
- High variance model, overfitting on training data  
- You care more about prediction accuracy than feature selection  

---

## 3. Lasso Regression (L1 Regularization)

Lasso adds an **L1 penalty** (absolute value of coefficients) to the loss.

### 3.1 Objective Function

```text
J_lasso = MSE + Œ± * Œ£ |Œ≤j|
```

Intuition:

- L1 penalty is ‚Äúsharper‚Äù than L2  
- Strongly encourages some coefficients to be **exactly zero**  
- Acts as **automatic feature selection**  

### 3.2 Effect on Coefficients

- Some coefficients become **exactly zero**  
- Model uses only a **subset of features**  
- Great for high-dimensional data where many features are irrelevant  

### 3.3 When Lasso Helps

- You suspect many features are **irrelevant or redundant**  
- You want a **simpler, sparser model**  
- You care about **interpretability** via feature selection  

---

## 4. Elastic Net (Combination of L1 + L2)

Elastic Net combines the strengths of Ridge and Lasso.

### 4.1 Objective Function

```text
J_elastic = MSE + Œ± * [ l1_ratio * Œ£ |Œ≤j| + (1 - l1_ratio) * Œ£ (Œ≤j^2) ]
```

Where:

- `Œ±` ‚Äì overall regularization strength  
- `l1_ratio` ‚Äì how much L1 vs L2 (between 0 and 1)

Intuition:

- L1 ‚Üí sparsity (feature selection)  
- L2 ‚Üí stability (handles correlated features)  
- Elastic Net works well when:
  - Features are correlated  
  - You also want feature selection  

---

## 5. Comparing Ridge, Lasso, and Elastic Net

### 5.1 Behavior Summary

| Method       | Penalty (Intuition)                        | Coefficients                 | Best Use Case                               |
|-------------|---------------------------------------------|------------------------------|---------------------------------------------|
| Ridge       | L2 ‚Äì sum of squared coefficients           | Shrunk, rarely exactly zero  | Multicollinearity, all features useful      |
| Lasso       | L1 ‚Äì sum of absolute coefficients          | Many become exactly zero     | Feature selection, sparse/simple models     |
| Elastic Net | L1 + L2 ‚Äì mix of absolute & squared terms  | Shrunk, some exactly zero    | Correlated features + sparsity needed       |


### 5.2 Practical Heuristics

- Start with **Ridge** when:
  - You have many correlated predictors  
  - You want a robust model without dropping features  

- Try **Lasso** when:
  - You have many features, some likely irrelevant  
  - You want a compact, explainable model  

- Use **Elastic Net** when:
  - There are groups of correlated features  
  - You want both stability and sparsity  

---

## 6. Role of Œ± (Regularization Strength)

The `Œ±` (alpha) parameter controls how strong the penalty is:

- `Œ± = 0` ‚Üí equivalent to plain Linear Regression (no regularization)  
- Small `Œ±` ‚Üí light regularization (coefficients close to OLS)  
- Large `Œ±` ‚Üí strong regularization (coefficients heavily shrunk)

### 6.1 Effect of Too Small Œ±

- Model behaves like unregularized Linear Regression  
- Risk of overfitting remains  

### 6.2 Effect of Too Large Œ±

- Coefficients get overly small  
- Underfitting ‚Äì model becomes too simple, misses patterns  

The sweet spot is usually found via **cross-validation**.

---

## 7. Feature Scaling

Ridge, Lasso, and Elastic Net are **sensitive to feature scale**.

- Features with larger scales can dominate the penalty  
- Always scale features before applying regularized linear models  

Common choices:

- Standardization: `(x - mean) / std`  
- Min‚ÄìMax scaling: `(x - min) / (max - min)`  

In scikit-learn, use `Pipeline` with `StandardScaler` + `Ridge`/`Lasso`/`ElasticNet`.

---

## 8. Hyperparameter Tuning

Regularized models usually have at least one key hyperparameter:

- Ridge: `alpha`  
- Lasso: `alpha`  
- Elastic Net: `alpha` and `l1_ratio`

### 8.1 Typical Tuning Strategy

- Try log-spaced values of `alpha`, e.g.  
  `[0.0001, 0.001, 0.01, 0.1, 1, 10, 100]`  
- Use cross-validation (e.g., `GridSearchCV`, `RandomizedSearchCV`)  
- Evaluate using:
  - MSE / RMSE  
  - R¬≤ / Adjusted R¬≤  
  - Business-specific evaluation  

### 8.2 scikit-learn Helpers

- `RidgeCV` ‚Äì Ridge with built-in cross-validation  
- `LassoCV` ‚Äì Lasso with built-in cross-validation  
- `ElasticNetCV` ‚Äì Elastic Net with built-in cross-validation  

These models automatically search for the best `alpha` (and `l1_ratio` for Elastic Net).

---

## 9. Evaluation Metrics

Ridge and Lasso are still **regression models**, so we use standard regression metrics:

- **MSE (Mean Squared Error)**  
- **RMSE (Root Mean Squared Error)**  
- **MAE (Mean Absolute Error)**  
- **R¬≤ (Coefficient of Determination)**  

Some notes:

- Regularized models may have slightly **higher training error** than OLS  
- But they usually have **better test error** due to reduced overfitting  
- Compare models (OLS vs Ridge vs Lasso) primarily on **validation / test performance**, not training error  

---

## 10. Interpretation of Coefficients

### 10.1 Ridge

- Coefficients are smaller than in OLS  
- All features typically remain in the model  
- Interpretation similar to Linear Regression:
  - ‚ÄúHolding other variables constant, a 1-unit increase in `xj` changes `y` by `Œ≤j` units on average.‚Äù  
- But individual coefficient magnitudes are less important than **overall prediction quality**.

### 10.2 Lasso

- Some coefficients are exactly zero  
- Features with non-zero coefficients are ‚Äúselected‚Äù as important  
- This makes Lasso helpful for:
  - Simplifying models  
  - Understanding which features are driving predictions  

Caution: when features are highly correlated, Lasso may pick **one arbitrary feature** from a group and drop the rest.

---

## 11. When Ridge & Lasso Work Well (and When They Don‚Äôt)

### 11.1 They Work Well When

- You have many features compared to samples  
- There is multicollinearity (correlated predictors)  
- You want to prevent overfitting  
- You care more about generalization than exact OLS fit  

Lasso/Elastic Net:

- Shine in high-dimensional problems (e.g., text data, many dummies)  
- Useful when you want **feature selection built into the model**  

### 11.2 They Struggle When

- Relationship between features and target is highly non-linear and you haven‚Äôt engineered good features  
- There are complex interactions that are not included  
- Data is extremely noisy and even strong regularization can‚Äôt save it  

In these cases, consider:

- Tree-based models (Random Forest, Gradient Boosting, XGBoost)  
- Kernel methods  
- Neural networks  

---

## 12. Real-World Applications

Ridge, Lasso, and Elastic Net are commonly used in:

- **Predictive modeling with many correlated variables**  
- **Text regression** (e.g., using bag-of-words or TF-IDF features)  
- **Genetics / bioinformatics** (huge number of features, small samples)  
- **Finance & risk modeling** (stable predictions required)  
- **Telecom**:
  - Predicting usage or spend  
  - Churn scores from many behavioral variables  
  - Building baseline models before moving to more complex algorithms  

---

## 13. Scikit-Learn Examples

### 13.1 Ridge Regression

```python
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

ridge_model = Pipeline([
    ("scaler", StandardScaler()),
    ("ridge", Ridge(alpha=1.0))
])

ridge_model.fit(X_train, y_train)
y_pred = ridge_model.predict(X_test)
```

---

### 13.2 Lasso Regression

```python
from sklearn.linear_model import Lasso
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

lasso_model = Pipeline([
    ("scaler", StandardScaler()),
    ("lasso", Lasso(alpha=0.1))
])

lasso_model.fit(X_train, y_train)
y_pred = lasso_model.predict(X_test)
```

---

### 13.3 Elastic Net

```python
from sklearn.linear_model import ElasticNet
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

enet_model = Pipeline([
    ("scaler", StandardScaler()),
    ("enet", ElasticNet(alpha=0.1, l1_ratio=0.5))
])

enet_model.fit(X_train, y_train)
y_pred = enet_model.predict(X_test)
```
