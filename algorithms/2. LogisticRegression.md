# ðŸ“˜ Logistic Regression

**Logistic Regression** is a supervised learning algorithm used primarily for **classification**. It models the probability that an input belongs to a particular class using the **logistic (sigmoid) function**.

Itâ€™s:

- Simple and fast  
- Interpretable  
- A very strong baseline for many binary and multiclass problems  

This section covers:

- Logistic Regression intuition  
- Estimating probabilities  
- Training and cost function  
- Decision boundaries  
- Softmax Regression (multiclass)  
- Regularization, evaluation, and practical nuances  

---

## 1. Logistic Regression â€“ Intuition

Instead of predicting a continuous value (like Linear Regression), Logistic Regression predicts **the probability of belonging to class 1** (e.g., churn vs non-churn).

Key idea:

- Work in **log-odds space (logit)** where the relationship between features and target is assumed **linear**  
- Map log-odds back to probability using the **sigmoid** function  

**Odds and log-odds:**

- odds = p / (1 - p)  
- logit(p) = log(p / (1 - p))  

Logistic Regression assumes:

```text
logit(p) = Î²0 + Î²1 * x1 + ... + Î²n * xn
```

---

## 2. Estimating Probabilities

Let:

```text
p = P(y = 1 | x)
```

The logistic (sigmoid) function maps a real-valued input z to a probability in (0, 1):

```text
sigmoid(z) = 1 / (1 + exp(-z))
```

where:

```text
z = Î²0 + Î²1 * x1 + ... + Î²n * xn
```

So the **hypothesis** is:

```text
y_hat = P(y = 1 | x) = sigmoid(Î²0 + Î²1 * x1 + ... + Î²n * xn)
```

**Interpretation of coefficients:**

- Each Î²j is the change in **log-odds** of class 1 for a unit change in xj, holding other features fixed.  
- exp(Î²j) is the **odds ratio**.

Logistic Regression is a **probabilistic classifier**: it outputs probabilities first, then class labels.

---

## 3. Model & Sigmoid Function

### Sigmoid Function

```text
sigmoid(z) = 1 / (1 + exp(-z))
```

Properties:

- Range: (0, 1)  
- sigmoid(0) = 0.5  
- As z -> +âˆž, sigmoid(z) -> 1  
- As z -> -âˆž, sigmoid(z) -> 0  

### Hypothesis Function

```text
z      = Î²0 + Î²1 * x1 + ... + Î²n * xn
y_hat  = sigmoid(z)
       = 1 / (1 + exp(-z))
```

This is what we use to **estimate probabilities** at prediction time.

---

## 4. Training and Cost Function (Log-Loss / Cross-Entropy)

Using MSE for Logistic Regression is not ideal. The standard loss is **log-loss (binary cross-entropy)**:

```text
J(Î²) = - (1 / m) * Î£[ y * log(y_hat) + (1 - y) * log(1 - y_hat) ]
```

where:

- m = number of samples  
- y âˆˆ {0, 1}  
- y_hat = predicted probability for class 1  

**Why log-loss?**

- Strongly penalizes confident wrong predictions (e.g., y = 0, y_hat = 0.99)  
- Leads to a **convex** optimization problem â†’ single global minimum  

### Gradient of the Cost Function

For parameter Î²j:

```text
âˆ‚J/âˆ‚Î²j = (1 / m) * Î£ (y_hat - y) * xj
```

Gradient Descent update rule:

```text
Î²j := Î²j - Î± * (1 / m) * Î£ (y_hat - y) * xj
```

where Î± = learning rate.

This is very similar to Linear Regression GD, but with:

- y_hat computed using sigmoid(z)  
- log-loss instead of squared error  

---

## 5. Decision Boundaries

Once we have the predicted probability y_hat, we convert it to a hard class label:

```text
if y_hat >= 0.5 â†’ class = 1
else           â†’ class = 0
```

Since:

```text
y_hat = sigmoid(Î²^T x)
```

the threshold 0.5 corresponds to:

```text
Î²^T x = 0
```

So the **decision boundary** is the set of points where:

```text
Î²^T x = 0
```

Geometric interpretation:

- The decision boundary is a **hyperplane** in feature space.  
- Logistic Regression learns linear decision boundaries in the transformed feature space.  
- To model non-linear boundaries, we must add **polynomial or interaction features**.

### Threshold Tuning

Instead of using 0.5:

- Lower threshold â†’ more predicted positives (higher recall, lower precision)  
- Higher threshold â†’ fewer predicted positives (higher precision, lower recall)  

Threshold should be chosen based on **business cost** of FP vs FN (fraud, churn, medical, risk).

---

## 6. Regularization (L1, L2, Elastic Net)

Logistic Regression almost always uses **regularization** in practice.

### L2 Regularization (Ridge)

```text
J_reg = J + Î» * Î£ (Î²j^2)
```

- Shrinks coefficients smoothly  
- Keeps all features, but reduces their magnitude  
- Default for many implementations (penalty="l2")

### L1 Regularization (Lasso)

```text
J_reg = J + Î» * Î£ |Î²j|
```

- Encourages sparsity  
- Drives some coefficients exactly to zero  
- Performs **feature selection**

### Elastic Net

```text
J_reg = J + Î»1 * Î£ |Î²j| + Î»2 * Î£ (Î²j^2)
```

- Combines L1 and L2  
- Useful when features are correlated and some sparsity is desired  

**Always scale features** before regularized Logistic Regression.

---

## 7. Softmax Regression (Multinomial Logistic Regression)

For **multiclass** problems (K classes), Logistic Regression generalizes to **Softmax Regression**.

Each class k has parameter vector Î²k. The model predicts:

```text
P(y = k | x) = exp(Î²k^T x) / Î£_j exp(Î²j^T x)
```

The cost function is **multiclass cross-entropy**:

```text
J = - (1 / m) * Î£ Î£ 1{y = k} * log( P(y = k | x) )
```

Predicted class:

```text
y_hat = argmax_k P(y = k | x)
```

Common strategies:

- One-vs-Rest (OvR) â†’ train K binary classifiers  
- Multinomial (Softmax) â†’ single joint model over all classes  

---

## 8. Feature Engineering for Logistic Regression

Logistic Regression is very sensitive to feature design. It works best with:

- **Scaling** (StandardScaler, MinMaxScaler)  
- **Polynomial features** to capture non-linear boundaries  
- **Interaction terms** (e.g., age * income)  
- **Domain features** (ratios, flags, aggregates)  
- **Proper categorical encoding** (one-hot, target encoding with care)  

With good features, Logistic Regression can match or beat more complex models.

---

## 9. Evaluation Metrics

For classification, especially with imbalance, we need more than accuracy.

- **Accuracy** â€“ overall correctness, but misleading when classes are imbalanced  
- **Precision** â€“ of predicted positives, how many are actually positive  
- **Recall** â€“ of actual positives, how many are captured  
- **F1-score** â€“ harmonic mean of precision and recall  
- **ROC-AUC** â€“ measures ranking quality of scores across thresholds  
- **PR-AUC** â€“ especially useful for rare positive classes  

Choose metrics based on **business context** and error costs.

---

## 10. Handling Class Imbalance

Common in fraud, churn, risk, defaults, etc.

Techniques:

- Use class_weight="balanced" in scikit-learn  
- Manually set class weights  
- Oversample minority (Random, SMOTE)  
- Undersample majority  
- Tune classification threshold  

Logistic Regression + good imbalance handling is a very strong baseline.

---

## 11. Assumptions of Logistic Regression

| Assumption               | Meaning                                      | Why It Matters                                | How to Check                      |
|--------------------------|----------------------------------------------|-----------------------------------------------|-----------------------------------|
| Linearity in log-odds    | log(p/(1-p)) is linear in features          | Non-linearity can hurt fit and calibration    | Boxâ€“Tidwell, residual analysis    |
| Independence             | Observations are independent                | Correlated samples distort standard errors    | Study design, time component check |
| No multicollinearity     | Features not highly correlated              | Coefficients unstable, large variance         | VIF, correlation matrix           |
| Sufficient sample size   | Enough events per predictor                 | Prevents overfitting, unstable coefficients   | Rule of thumb: 10â€“20 events/feature |

Logistic Regression **does NOT require**:

- Normally distributed features  
- Equal variances across features  

---

## 12. When Logistic Regression Works Well

- Classes are roughly separable by a linear boundary in feature space  
- You need **interpretability** (odds ratios, sign and strength of effects)  
- Tabular data with moderate number of features  
- You care about **probabilities**, not just labels  

---

## 13. When It Fails

- Strong non-linear decision boundaries  
- Complex interactions not captured in features  
- Extremely high-dimensional sparse inputs without proper regularization  
- Severe imbalance with no class weighting or resampling  

Good alternatives:

- Tree-based models (Random Forest, XGBoost, LightGBM)  
- Kernel SVM  
- Neural networks  

---

## 14. Real-World Applications

Logistic Regression is heavily used in:

- Fraud detection  
- Churn prediction  
- Credit risk modeling  
- Medical diagnosis  
- Marketing response modeling  
- Telecom propensity / default risk models  

Many regulated industries prefer it due to **interpretability** and stability.

---

## 15. Scikit-Learn Example (Binary, with Probabilities)

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score

model = LogisticRegression(
    penalty="l2",
    solver="lbfgs",
    max_iter=1000
)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

print(classification_report(y_test, y_pred))
print("ROC-AUC:", roc_auc_score(y_test, y_proba))
```

---

## 16. Scikit-Learn Example (Softmax / Multinomial)

```python
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(
    multi_class="multinomial",
    solver="lbfgs",
    max_iter=1000
)

model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```
