# ðŸ“˜ Logistic Regression

**Logistic Regression** is a supervised learning algorithm used primarily for **classification**. It models the probability that an input belongs to a particular class using the **logistic (sigmoid) function**.  

Itâ€™s:
- Simple and fast  
- Interpretable  
- A very strong baseline for many binary and multiclass problems  

This section covers:

- Logistic Regression intuition  
- Estimating probabilities  
- Training and cost function  
- Decision boundaries  
- Softmax Regression (multiclass)  
- Regularization, evaluation, and practical nuances  

---

## 1. Logistic Regression â€“ Intuition

Instead of predicting a continuous value (like Linear Regression), Logistic Regression predicts **the probability of belonging to class 1** (e.g., churn vs non-churn).

Key idea:
- Work in **log-odds space (logit)** where the relationship between features and target is assumed **linear**
- Map log-odds back to probability using the **sigmoid** function

Log-odds (logit):

```math
	ext{logit}(p) = \log\left(rac{p}{1 - p}ight)
```

Logistic Regression assumes:

```math
	ext{logit}(p) = eta_0 + eta_1 x_1 + \dots + eta_n x_n
```

---

## 2. Estimating Probabilities

Let \(p = P(y = 1 \mid x)\). The logistic (sigmoid) function maps linear output \(z\) to a probability:

```math
p = \sigma(z) = rac{1}{1 + e^{-z}}
```

where

```math
z = eta_0 + eta_1 x_1 + \dots + eta_n x_n
```

So the **hypothesis** is:

```math
\hat{y} = P(y=1 \mid x) = \sigma(eta_0 + eta_1 x_1 + \dots + eta_n x_n)
```

Interpretation of coefficients:
- Each \(eta_j\) is the change in **log-odds** of class 1 for a unit change in \(x_j\), holding other features fixed.
- Exponentiating gives **odds ratio**: \(e^{eta_j}\).

Logistic Regression is therefore a **probabilistic classifier**: it outputs probabilities first, then class labels.

---

## 3. Model & Sigmoid Function

### Sigmoid Function

```math
\sigma(z) = rac{1}{1 + e^{-z}}
```

Properties:
- Range: (0, 1)  
- \(\sigma(0) = 0.5\)  
- As \(z 	o +\infty\), \(\sigma(z) 	o 1\)  
- As \(z 	o -\infty\), \(\sigma(z) 	o 0\)  

### Hypothesis Function

```math
\hat{y} = \sigma(eta^T x) = rac{1}{1 + e^{-eta^T x}}
```

This is what we use to **estimate probabilities** at prediction time.

---

## 4. Training and Cost Function (Log-Loss / Cross-Entropy)

If we tried MSE here, the cost surface would not be convex. For Logistic Regression, we use **log-loss (cross-entropy)**:

```math
J(eta) = - rac{1}{m} \sum_{i=1}^{m}
\left[ y^{(i)} \log(\hat{y}^{(i)})
+ (1 - y^{(i)}) \log(1 - \hat{y}^{(i)}) ight]
```

Where:
- \(m\) = number of samples  
- \(y^{(i)} \in \{0, 1\}\)  
- \(\hat{y}^{(i)}\) = modelâ€™s predicted probability for sample i  

**Why log-loss?**
- Strongly penalizes confident wrong predictions (e.g., predicting 0.99 when true label is 0)  
- Leads to a **convex optimization problem** for Logistic Regression â†’ single global minimum  

### Gradient of the Cost Function

The gradient wrt \(eta_j\):

```math
rac{\partial J}{\partial eta_j}
= rac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)}) x_j^{(i)}
```

Update rule (Gradient Descent):

```math
eta_j := eta_j - lpha \cdot
rac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)}) x_j^{(i)}
```

Where \(lpha\) is the learning rate.

This is conceptually similar to Linear Regression GD, except the predictions are passed through a **sigmoid** and cost is **log-loss**.

---

## 5. Decision Boundaries

Once we have the predicted probability \(\hat{y}\), we convert it to a class label:

```math
\hat{y} \geq 0.5 \Rightarrow 	ext{class } 1,\quad
\hat{y} < 0.5 \Rightarrow 	ext{class } 0
```

This corresponds to:

```math
eta^T x \geq 0 \Rightarrow 	ext{class } 1
```

So the **decision boundary** is defined by:

```math
eta^T x = 0
```

Geometric interpretation:
- The decision boundary is a **hyperplane** in feature space  
- Logistic Regression can only learn **linear decision boundaries** in the original feature space  
- Non-linear boundaries require **feature engineering** (polynomial features, interactions, etc.)  

### Threshold Tuning

Instead of using 0.5:
- Lower threshold â†’ more positives (higher recall, lower precision)  
- Higher threshold â†’ fewer positives (higher precision, lower recall)  

Threshold choice should be driven by **business cost** of FP vs FN (fraud, churn, medical).

---

## 6. Regularization (L1, L2, Elastic Net)

Logistic Regression often includes **regularization** to reduce overfitting and handle high-dimensional data.

### L2 Regularization (Ridge)

```math
J_{reg} = J + \lambda \sum_{j=1}^{n} eta_j^2
```

- Shrinks coefficients smoothly  
- Keeps all features, but reduces their magnitude  
- Default in many implementations  

### L1 Regularization (Lasso)

```math
J_{reg} = J + \lambda \sum_{j=1}^{n} |eta_j|
```

- Encourages sparsity  
- Drives some coefficients exactly to zero  
- Performs **feature selection**  

### Elastic Net

```math
J_{reg} = J + \lambda_1 \sum |eta_j| + \lambda_2 \sum eta_j^2
```

- Balances L1 and L2  
- Useful when features are correlated and some sparsity is desired  

Proper **feature scaling** is important for all regularized models.

---

## 7. Softmax Regression (Multinomial Logistic Regression)

When there are **more than 2 classes**, Logistic Regression generalizes to **Softmax Regression** (also called Multinomial Logistic Regression).

Given K classes, each with parameter vector \(eta_k\):

```math
P(y = k \mid x) =
rac{e^{eta_k^T x}}{\sum_{j=1}^{K} e^{eta_j^T x}}
```

The cost function becomes **multiclass cross-entropy**:

```math
J(\{eta_k\}) =
- rac{1}{m} \sum_{i=1}^{m}
\sum_{k=1}^{K}
\mathbb{1}\{y^{(i)} = k\}
\log P(y^{(i)} = k \mid x^{(i)})
```

Softmax outputs a **probability distribution** over all classes.  
Predicted class is the one with maximum probability:

```math
\hat{y} = rg\max_k P(y = k \mid x)
```

Two common implementation styles:
- **One-vs-Rest (OvR)** â†’ K binary classifiers  
- **Multinomial / Softmax** â†’ Single joint model over all classes  

---

## 8. Feature Engineering for Logistic Regression

Logistic Regression can be very powerful with the right features:

- **Scaling** â†’ speeds up optimization  
- **Polynomial features** â†’ capture non-linear decision boundaries  
- **Interaction terms** â†’ model combined feature effects  
- **Domain-specific features** (ratios, flags, aggregates)  
- **Categorical encoding** (one-hot, target encoding carefully)  

Even though the model is linear in features, thoughtfully engineered features make the **effective** boundary non-linear in original space.

---

## 9. Evaluation Metrics

Accuracy alone is insufficient, especially under class imbalance. Use:

- **Accuracy** â€“ quick sanity check  
- **Precision** â€“ quality of positive predictions  
- **Recall** â€“ coverage of actual positives  
- **F1-score** â€“ balance of precision & recall  
- **ROC-AUC** â€“ ranking ability across thresholds  
- **PR-AUC** â€“ more informative for imbalanced data  

Choose metrics aligned with **business objectives** (fraud, churn, defaults, etc.).

---

## 10. Handling Class Imbalance

Class imbalance is common in real-world applications.

Approaches include:
- `class_weight="balanced"` in sklearn  
- Manual class weights based on inverse class frequency  
- Oversampling minority (Random, SMOTE)  
- Undersampling majority  
- Threshold tuning based on cost  

Logistic Regression + proper imbalance handling is a strong baseline.

---

## 11. Assumptions of Logistic Regression

| Assumption | Meaning | Why It Matters | How to Check |
|-----------|---------|----------------|--------------|
| Linearity in log-odds | Log(p/(1-p)) is linear in features | Non-linearity hurts fit & calibration | Boxâ€“Tidwell, residual plots |
| Independence | Observations are independent | Correlated data invalidates standard errors | Study design, time series checks |
| No multicollinearity | Features not highly correlated | Coefficients may blow up / flip signs | VIF, correlation matrix |
| Sufficient sample size | Enough events per predictor | Prevents unstable estimates | Rule: 10â€“20 events per feature |

Logistic Regression **does not** require:
- Normally distributed features  
- Equal variance features  

---

## 12. When Logistic Regression Works Well

- Approximate linear separation in feature space  
- Need for **interpretability** (odds ratios)  
- Moderate number of features  
- Tabular data with reasonable feature engineering  
- Probabilistic outputs are needed, not just labels  

---

## 13. When It Fails

- Strongly non-linear class boundaries  
- Heavy interactions not captured in features  
- Extremely high-dimensional sparse data (without regularization)  
- Severe class imbalance without proper handling  

Alternatives:
- Tree-based models (Random Forest, Gradient Boosting)  
- Kernel SVM  
- Neural networks  

---

## 14. Real-World Applications

Logistic Regression is heavily used in:

- **Fraud detection**  
- **Churn prediction**  
- **Credit scoring & risk models**  
- **Medical diagnosis**  
- **Marketing response modeling**  
- **Telecom**: voluntary/involuntary churn, product propensity, default risk  

Its interpretability and probability outputs make it popular in **regulated domains**.

---

## 15. Scikit-Learn Example (Binary, with Probabilities)

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score

model = LogisticRegression(
    penalty="l2",
    solver="lbfgs",
    max_iter=1000
)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

print(classification_report(y_test, y_pred))
print("ROC-AUC:", roc_auc_score(y_test, y_proba))
```

---

## 16. Scikit-Learn Example (Softmax / Multinomial)

```python
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(
    multi_class="multinomial",
    solver="lbfgs",
    max_iter=1000
)

model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```
