
# üìò Logistic Regression

**Logistic Regression** is a supervised learning algorithm used primarily for **classification**. It models the probability that an input belongs to a particular class using the **logistic (sigmoid) function**.

It‚Äôs:

* Simple and fast
* Interpretable
* A very strong baseline for many binary and multiclass problems

This section covers:

* Logistic Regression intuition
* Estimating probabilities
* Training and cost function
* Decision boundaries
* Softmax Regression (multiclass)
* Regularization, evaluation, and practical nuances

---

## 1. Logistic Regression ‚Äì Intuition

Instead of predicting a continuous value (like Linear Regression), Logistic Regression predicts **the probability of belonging to class 1** (e.g., churn vs non-churn).

Key idea:

* Work in **log-odds space (logit)** where the relationship between features and target is assumed **linear**
* Map log-odds back to probability using the **sigmoid** function

**Odds and log-odds:**

* `odds = p / (1 - p)`
* `logit(p) = log(p / (1 - p))`

Logistic Regression assumes:

```text
logit(p) = Œ≤0 + Œ≤1 * x1 + ... + Œ≤n * xn
```

---

## 2. Estimating Probabilities

Let:

```text
p = P(y = 1 | x)
```

The logistic (sigmoid) function maps a real-valued input `z` to a probability in (0, 1):

```text
sigmoid(z) = 1 / (1 + exp(-z))
```

where:

```text
z = Œ≤0 + Œ≤1 * x1 + ... + Œ≤n * xn
```

So the **hypothesis** is:

```text
y_hat = P(y = 1 | x) = sigmoid(Œ≤0 + Œ≤1 * x1 + ... + Œ≤n * xn)
```

**Interpretation of coefficients:**

* Each `Œ≤j` is the change in **log-odds** of class 1 for a unit change in `xj`, holding other features fixed.
* `exp(Œ≤j)` is the **odds ratio**.

Logistic Regression is a **probabilistic classifier**: it outputs probabilities first, then class labels.

---

## 3. Model & Sigmoid Function

### Sigmoid Function

```text
sigmoid(z) = 1 / (1 + exp(-z))
```

Properties:

* Range: (0, 1)
* `sigmoid(0) = 0.5`
* As `z -> +‚àû`, `sigmoid(z) -> 1`
* As `z -> -‚àû`, `sigmoid(z) -> 0`

### Hypothesis Function

```text
z      = Œ≤0 + Œ≤1 * x1 + ... + Œ≤n * xn
y_hat  = sigmoid(z)
       = 1 / (1 + exp(-z))
```

This is what we use to **estimate probabilities** at prediction time.

---

## 4. Training and Cost Function (Log-Loss / Cross-Entropy)

Using MSE for Logistic Regression is not ideal. The standard loss is **log-loss (binary cross-entropy)**:

```text
J(Œ≤) = - (1 / m) * Œ£[ y * log(y_hat) + (1 - y) * log(1 - y_hat) ]
```

where:

* `m` = number of samples
* `y ‚àà {0, 1}`
* `y_hat` = predicted probability for class 1

**Why log-loss?**

* Strongly penalizes confident wrong predictions (e.g., `y = 0`, `y_hat = 0.99`)
* Leads to a **convex** optimization problem ‚Üí single global minimum

### Gradient of the Cost Function

For parameter `Œ≤j`:

```text
‚àÇJ/‚àÇŒ≤j = (1 / m) * Œ£ (y_hat - y) * xj
```

Gradient Descent update rule:

```text
Œ≤j := Œ≤j - Œ± * (1 / m) * Œ£ (y_hat - y) * xj
```

where `Œ±` = learning rate.

This is very similar to Linear Regression GD, but with:

* `y_hat` computed using `sigmoid(z)`
* log-loss instead of squared error

---

## 5. Decision Boundaries

Once we have the predicted probability `y_hat`, we convert it to a hard class label:

```text
if y_hat >= 0.5 ‚Üí class = 1
else           ‚Üí class = 0
```

Since:

```text
y_hat = sigmoid(Œ≤^T x)
```

the threshold `0.5` corresponds to:

```text
Œ≤^T x = 0
```

So the **decision boundary** is the set of points where:

```text
Œ≤^T x = 0
```

Geometric interpretation:

* The decision boundary is a **hyperplane** in feature space.
* Logistic Regression learns linear decision boundaries in the transformed feature space.
* To model non-linear boundaries, we must add **polynomial or interaction features**.

### Threshold Tuning

Instead of using 0.5:

* Lower threshold ‚Üí more predicted positives (higher recall, lower precision)
* Higher threshold ‚Üí fewer predicted positives (higher precision, lower recall)

Threshold should be chosen based on **business cost** of FP vs FN (fraud, churn, medical, risk).

---

## 6. Regularization (L1, L2, Elastic Net)

Logistic Regression almost always uses **regularization** in practice.

### L2 Regularization (Ridge)

```text
J_reg = J + Œª * Œ£ (Œ≤j^2)
```

* Shrinks coefficients smoothly
* Keeps all features, but reduces their magnitude
* Default for many implementations (`penalty="l2"`)

### L1 Regularization (Lasso)

```text
J_reg = J + Œª * Œ£ |Œ≤j|
```

* Encourages sparsity
* Drives some coefficients exactly to zero
* Performs **feature selection**

### Elastic Net

```text
J_reg = J + Œª1 * Œ£ |Œ≤j| + Œª2 * Œ£ (Œ≤j^2)
```

* Combines L1 and L2
* Useful when features are correlated and some sparsity is desired

**Always scale features** before regularized Logistic Regression.

---

## 7. Softmax Regression (Multinomial Logistic Regression)

For **multiclass** problems (K classes), Logistic Regression generalizes to **Softmax Regression**.

Each class `k` has parameter vector `Œ≤k`. The model predicts:

```text
P(y = k | x) = exp(Œ≤k^T x) / Œ£_j exp(Œ≤j^T x)
```

The cost function is **multiclass cross-entropy**:

```text
J = - (1 / m) * Œ£ Œ£ 1{y = k} * log( P(y = k | x) )
```

Predicted class:

```text
y_hat = argmax_k P(y = k | x)
```

Common strategies:

* **One-vs-Rest (OvR)** ‚Üí train K binary classifiers
* **Multinomial (Softmax)** ‚Üí single joint model over all classes

---

## 8. Feature Engineering for Logistic Regression

Logistic Regression is very sensitive to feature design. It works best with:

* **Scaling** (`StandardScaler`, `MinMaxScaler`)
* **Polynomial features** to capture non-linear boundaries
* **Interaction terms** (e.g., `age * income`)
* **Domain features** (ratios, flags, aggregates)
* **Proper categorical encoding** (one-hot, target encoding with care)

With good features, Logistic Regression can match or beat more complex models.

---

## 9. Evaluation Metrics

For classification models like Logistic Regression, especially when classes are imbalanced, we need more than just accuracy to judge performance.

### 9.1 Confusion Matrix (Base for All Metrics)

Most metrics come from the **confusion matrix**:

* **TP (True Positives)** ‚Äì model predicted 1, and actual is 1
* **TN (True Negatives)** ‚Äì model predicted 0, and actual is 0
* **FP (False Positives)** ‚Äì model predicted 1, but actual is 0
* **FN (False Negatives)** ‚Äì model predicted 0, but actual is 1

Think of ‚Äúpositive‚Äù as the event of interest (churn, fraud, default, disease, etc.).

---

### 9.2 Accuracy

> Out of all predictions, how many did we get right?

```text
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

* Good when classes are **balanced**.
* Misleading when the positive class is **rare**.

Example: If only 1% of customers churn, a dumb model that predicts ‚Äúno churn‚Äù for everyone gets 99% accuracy but is useless.

Use accuracy as a **quick sanity check**, not the only metric.

---

### 9.3 Precision

> Of all the cases the model predicted as positive, how many were actually positive?

```text
Precision = TP / (TP + FP)
```

* High precision = **few false alarms**.
* Useful when **false positives are expensive**.

Example: approving risky loans, flagging customers as ‚ÄúVIP upgrade candidates‚Äù.

If your business hates ‚Äúcrying wolf‚Äù too often, focus on **precision**.

---

### 9.4 Recall (Sensitivity)

> Of all the actual positive cases, how many did the model correctly catch?

```text
Recall = TP / (TP + FN)
```

* High recall = **few misses**.
* Useful when **missing a positive case is very costly**.

Example: missing fraud, missing a serious disease, missing a high-risk churner.

If your business hates ‚Äúmissing real problems‚Äù, focus on **recall**.

---

### 9.5 F1-Score

> Single score that balances precision and recall.

```text
F1 = 2 * (Precision * Recall) / (Precision + Recall)
```

* High F1 means both precision and recall are reasonably good.
* Useful when:

  * Classes are **imbalanced**
  * You want a balance between **catching positives** and **avoiding false alarms**

If precision and recall are both important, use **F1** as your primary metric.

---

### 9.6 ROC-AUC (Receiver Operating Characteristic ‚Äì Area Under Curve)

Logistic Regression outputs a **score/probability**, and you can vary the classification threshold (0.3, 0.5, 0.7, etc.).

ROC curve plots:

* X-axis: **False Positive Rate (FPR)**
* Y-axis: **True Positive Rate (TPR) = Recall**

**AUC (Area Under Curve)** tells you how well the model ranks positive cases higher than negative ones across all possible thresholds.

* AUC close to **1.0** ‚Üí model ranks very well
* AUC around **0.5** ‚Üí model is like random guessing

Use **ROC-AUC** when:

* Classes are **not extremely imbalanced**
* You care about overall ranking quality across thresholds

---

### 9.7 PR-AUC (Precision‚ÄìRecall Area Under Curve)

For highly imbalanced datasets, ROC-AUC can look ‚Äúgood‚Äù even if performance on the minority class is poor.

PR curve plots:

* X-axis: **Recall**
* Y-axis: **Precision**

**PR-AUC** focuses only on the **positive class** and how precision and recall trade off as the threshold changes.

Use **PR-AUC** when:

* Positive class is **rare** (fraud, churn, defaults)
* You really care about performance on the **minority (positive) class**

---

### 9.8 Which Metric Should I Use?

Some quick guidance:

* **Balanced dataset, both classes equally important** ‚Üí Accuracy, F1, ROC-AUC
* **False positives are expensive** (e.g., wrongly flagging loyal customers as risky) ‚Üí **Precision**
* **False negatives are expensive** (e.g., missing churners, fraud cases, high-risk patients) ‚Üí **Recall**
* **Imbalanced data, rare positive class** ‚Üí **F1**, **PR-AUC**, and **Recall**
* **Rank-order use case** (top N customers to target, top N risky accounts) ‚Üí **ROC-AUC**, sometimes **PR-AUC**

Always connect metric choice to **business impact**:

* What is worse: a **false alarm** or a **missed case**?
* Are we okay with more alerts if it means catching more real issues?

---

## 10. Handling Class Imbalance

Common in fraud, churn, risk, defaults, etc.

Techniques:

* Use `class_weight="balanced"` in scikit-learn
* Manually set class weights
* Oversample minority (Random, SMOTE)
* Undersample majority
* Tune classification threshold

Logistic Regression + good imbalance handling is a very strong baseline.

---

## 11. Assumptions of Logistic Regression

| Assumption             | Meaning                              | Why It Matters                              | How to Check                        |
| ---------------------- | ------------------------------------ | ------------------------------------------- | ----------------------------------- |
| Linearity in log-odds  | `log(p/(1-p))` is linear in features | Non-linearity can hurt fit and calibration  | Box‚ÄìTidwell, residual analysis      |
| Independence           | Observations are independent         | Correlated samples distort standard errors  | Study design, time component check  |
| No multicollinearity   | Features not highly correlated       | Coefficients unstable, large variance       | VIF, correlation matrix             |
| Sufficient sample size | Enough events per predictor          | Prevents overfitting, unstable coefficients | Rule of thumb: 10‚Äì20 events/feature |

Logistic Regression **does NOT require**:

* Normally distributed features
* Equal variances across features

---

## 12. When Logistic Regression Works Well

* Classes are roughly separable by a **linear boundary** in feature space
* You need **interpretability** (odds ratios, sign and strength of effects)
* Tabular data with a moderate number of features
* You care about **probabilities**, not just labels

---

## 13. When It Fails

* Strong non-linear decision boundaries
* Complex interactions not captured in features
* Extremely high-dimensional sparse inputs without proper regularization
* Severe imbalance with no class weighting or resampling

Good alternatives:

* Tree-based models (Random Forest, XGBoost, LightGBM)
* Kernel SVM
* Neural networks

---

## 14. Real-World Applications

Logistic Regression is heavily used in:

* Fraud detection
* Churn prediction
* Credit risk modeling
* Medical diagnosis
* Marketing response modeling
* Telecom propensity / default risk models

Many regulated industries prefer it due to **interpretability** and stability.

---

## 15. Scikit-Learn Example (Binary, with Probabilities)

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score

model = LogisticRegression(
    penalty="l2",
    solver="lbfgs",
    max_iter=1000
)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

print(classification_report(y_test, y_pred))
print("ROC-AUC:", roc_auc_score(y_test, y_proba))
```

---

## 16. Scikit-Learn Example (Softmax / Multinomial)

```python
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(
    multi_class="multinomial",
    solver="lbfgs",
    max_iter=1000
)

model.fit(X_train, y_train)
y_pred = model.predict(X_test)
``
