# üìò Decision Trees

A **Decision Tree** is a supervised learning algorithm that splits the feature space into regions using a series of if‚Äìelse rules.

- Works for **classification** and **regression**  
- Easy to **interpret** and **visualize**  
- Handles **non-linear** relationships and **mixed** feature types (numeric + categorical)  

Decision Trees are also the base learner for powerful ensembles like **Random Forests** and **Gradient Boosted Trees**.

---

## 1. Training and Visualizing a Decision Tree

Training a decision tree means:

1. Start with all training data at the **root**.  
2. At each node, choose:
   - A feature  
   - A split rule (threshold or category split)  
   that best separates the target values according to a **cost function**.  
3. Split the data into child nodes.  
4. Repeat recursively on each child node until a **stopping condition** is met.

Common stopping conditions:

- Maximum depth reached  
- Too few samples in a node  
- Node is pure (all samples same label)  
- Maximum number of leaf nodes reached  

Basic scikit-learn example (classification):

```python
from sklearn.tree import DecisionTreeClassifier

tree_clf = DecisionTreeClassifier(
    criterion="gini",       # or "entropy"
    max_depth=None,
    random_state=42
)
tree_clf.fit(X_train, y_train)
```

To visualize:

- `sklearn.tree.plot_tree(tree_clf, filled=True, feature_names=..., class_names=...)`  
- Or use `export_graphviz` and Graphviz for publication-quality diagrams.

Visualization helps you see:

- Which features are used near the top (most important)  
- How thresholds are chosen  
- What rules lead to each prediction  

---

## 2. How Decision Trees Split Features

At each node, a Decision Tree must answer:

> Which feature, and which split on that feature, best separates the target values?

The process is similar for **numeric** and **categorical** features, but the mechanics differ a bit.  
In all cases, the tree uses a **cost function** to measure how good a split is.

---

### 2.1 Splitting Numeric Features

For a numeric feature (e.g., `age`, `income`, `call_duration`), the tree considers splits of the form:

```text
feature_j <= threshold
```

**How a threshold is chosen (conceptually):**

1. Select a numeric feature `feature_j`.  
2. Sort the samples by `feature_j`.  
3. Consider candidate thresholds between consecutive unique values.  
4. For each threshold:
   - Left node: samples where `feature_j <= threshold`  
   - Right node: samples where `feature_j > threshold`  
   - Compute the **split cost** (weighted impurity or error).  
5. Repeat for all features, and pick the **feature + threshold** pair with the lowest cost.

**Pros (numeric splitting)**

- Captures **non-linear** relationships by stacking multiple splits.  
- Does **not require scaling/standardization**.  
- Very interpretable rules: `age <= 35`, `monthly_usage > 20 GB`.

**Cons (numeric splitting)**

- Splits are **axis-aligned** (one feature at a time), so complex curved boundaries need many nodes.  
- Trees can become **deep and complex** for granular numeric patterns ‚Üí overfitting risk.  
- Small changes in the data can change the chosen thresholds ‚Üí contributes to **instability**.

---

### 2.2 Splitting Categorical Features

For a categorical feature (e.g., `state`, `plan_type`, `segment`), the tree must decide **which categories go to which branch**.

Common strategies (implementation-dependent):

#### a) Binary split on a subset of categories

The tree finds a subset of categories that best separates the target:

```text
if plan_type in {"Premium", "Gold"}:
    go left
else:
    go right
```

The algorithm tries different subsets (or an efficient approximation) and picks the one with the lowest cost.

#### b) Order categories by target statistic

For binary classification or regression:

- Compute a statistic per category (e.g., mean target or probability of class 1).  
- Sort categories by that statistic.  
- Treat this ordering like a numeric variable and split using a threshold in this order.

#### c) One-hot encoding before the tree

In many practical pipelines:

- Convert each category into binary indicators (`is_state_CA`, `is_state_TX`, ‚Ä¶).  
- The tree then splits on these 0/1 features just like numeric features.

**Pros (categorical splitting)**

- Naturally incorporates **non-numeric** information (plan type, geography, segments).  
- Rules are intuitive:
  - `if plan_type in {"Basic", "Prepaid"} then ...`  
- With one-hot encoding, works seamlessly with most tree implementations.

**Cons (categorical splitting)**

- **High-cardinality** features (hundreds of categories) can:
  - Make very specific splits that **overfit**  
  - Increase computation time and memory  
- If raw category codes (0, 1, 2, ‚Ä¶) are used without one-hot, splits may reflect arbitrary numeric encoding, not real relationships.  
- Needs preprocessing decisions:
  - Grouping rare categories into ‚ÄúOther‚Äù  
  - Choosing encoding (one-hot, target encoding, etc.)

**Practical tips**

- Low/medium cardinality (say < 10‚Äì20 categories): one-hot or direct splits are usually fine.  
- Very high cardinality:
  - Group categories into meaningful buckets (regions, plan families).  
  - Or use regularized encodings (target/frequency encoding).

---

### 2.3 Cost Function for Splits (What the Tree Minimizes)

At each node, for each candidate split, the tree computes a **split cost**:

- **Classification** ‚Üí impurity-based cost (Gini or Entropy)  
- **Regression** ‚Üí variance/error-based cost (usually MSE)

The split with the **lowest cost** is chosen.

#### Classification split cost

Let:

- `n_total` = number of samples at the current node  
- Candidate split produces:
  - Left node with `n_left` samples, impurity `impurity_left`  
  - Right node with `n_right` samples, impurity `impurity_right`  

Then:

```text
cost_split = (n_left  / n_total) * impurity_left
           + (n_right / n_total) * impurity_right
```

The tree chooses the split with **minimum cost_split**  
(or equivalently, **maximum impurity reduction / information gain**).

#### Regression split cost

For regression, use something like **MSE** instead of impurity.

For a node with target values y1, y2, ..., yn and mean y_mean_node:

```text
MSE_node = (1 / n_node) * Œ£ (yi - y_mean_node)^2
```

For a candidate split:

```text
cost_split = (n_left  / n_total) * MSE_left
           + (n_right / n_total) * MSE_right
```

Again, pick the split with the **smallest cost_split**.

---

## 3. Making Predictions

To predict for a new sample:

1. Start at the **root node**.  
2. At each node, evaluate the split rule (e.g., `feature_3 <= 1.5`, `plan_type in {"Gold", "Platinum"}`).  
3. Follow the **left** or **right** child accordingly.  
4. Continue until you reach a **leaf**.  
5. The leaf stores the prediction:
   - Classification: majority class (and class distribution)  
   - Regression: average target value in that leaf  

Prediction time for one sample is proportional to the **depth** of the tree (typically small).

---

## 4. Estimating Class Probabilities

For classification trees, each leaf stores the **class frequency** of training samples that ended up there.

Example:

- Leaf has 80 samples:  
  - 60 of class 1  
  - 20 of class 0  

Then:

```text
P(y = 1 | leaf) = 60 / 80 = 0.75
P(y = 0 | leaf) = 20 / 80 = 0.25
Predicted class = 1 (majority)
```

In scikit-learn:

```python
proba = tree_clf.predict_proba(X_new)  # returns array of class probabilities
pred  = tree_clf.predict(X_new)       # returns predicted class labels
```

This is useful when:

- You need **probabilities** to feed into other systems (risk, pricing, ranking).  
- You want to **adjust decision thresholds** based on cost of errors.

---

## 5. The CART Training Algorithm

Most modern decision trees (including scikit-learn) implement **CART**:

> Classification And Regression Trees

Key properties:

- Always builds a **binary tree** (each split creates two children).  
- Uses a **greedy, top-down** strategy:
  - At each node, searches over all features and candidate splits.  
  - Chooses the best split based on impurity or error reduction.  
  - Does **not** reconsider splits made higher up in the tree.  
- Continues splitting until:
  - A stopping criterion is met, or  
  - The node is pure or too small.

Greedy splitting means the final tree is not globally optimal,  
but the algorithm is **fast** and works well in practice.

---

## 6. Computational Complexity

Let:

- `n` = number of samples  
- `d` = number of features  
- `D` = depth of the tree  

Rough complexity:

- **Training**:
  - Naive: `O(d * n^2)`  
  - With sorted thresholds (like scikit-learn): roughly `O(d * n * log n)`  

- **Prediction**:
  - Per sample: `O(D)` (follow one path from root to leaf)  
  - For all samples: `O(n * D)`

This makes trees **reasonably fast to train** and **very fast to predict**.

---

## 7. Gini Impurity or Entropy?

These are two common impurity measures for **classification**.

### 7.1 Gini Impurity

For a node with K classes and class probabilities p1, p2, ..., pK:

```text
Gini = 1 - Œ£ (pk^2)
```

- Gini = 0 ‚Üí node is pure (all samples same class)  
- Higher Gini ‚Üí more mixed node  

### 7.2 Entropy

```text
Entropy = - Œ£ (pk * log2(pk))
```

- Entropy = 0 ‚Üí node is pure  
- Higher Entropy ‚Üí more uncertainty  

### 7.3 Which One?

- Both usually yield **similar trees**.  
- **Gini**:
  - Slightly faster to compute  
  - Tends to isolate the most frequent class a bit more  
- **Entropy**:
  - Has a nice information-theoretic meaning (information gain)  

In scikit-learn:

```python
criterion="gini"    # default
# or
criterion="entropy"
```

For most use cases, **Gini is a good default**.

---

## 8. Regularization Hyperparameters

Decision Trees can easily **overfit** if unconstrained.  
Regularization controls tree complexity.

Important hyperparameters (scikit-learn style):

- `max_depth`  
  - Maximum depth of the tree.  
  - Smaller values ‚Üí simpler, more generalizable trees.

- `min_samples_split`  
  - Minimum samples required to **split** an internal node.  
  - Higher values ‚Üí fewer splits.

- `min_samples_leaf`  
  - Minimum samples required to be in a **leaf**.  
  - Useful to smooth predictions and reduce variance.

- `max_leaf_nodes`  
  - Maximum number of leaf nodes.  
  - Direct upper bound on tree size.

- `max_features`  
  - Max number of features to consider at each split.  
  - Randomizing features per split reduces variance and is used in ensembles.

- `ccp_alpha` (cost-complexity pruning)  
  - Post-pruning parameter.  
  - Higher `ccp_alpha` ‚Üí more pruning, smaller tree.

Typical workflow:

1. Start with a reasonably constrained tree (e.g., `max_depth`, `min_samples_leaf`).  
2. Use cross-validation to tune these hyperparameters.  
3. Balance **training performance** vs **validation performance** to avoid over/underfitting.

---

## 9. Decision Trees for Regression

Decision Trees can do **regression** (predict continuous values).

Differences from classification:

- Splits are chosen to minimize **error** (e.g., MSE) rather than class impurity.  
- Each leaf stores the **average target value** of the samples inside it.  

Example:

```python
from sklearn.tree import DecisionTreeRegressor

tree_reg = DecisionTreeRegressor(
    max_depth=4,
    random_state=42
)
tree_reg.fit(X_train, y_train)

y_pred = tree_reg.predict(X_test)
```

Behavior:

- Can model **non-linear** relationships and interactions  
- Predictions are **piecewise constant** (step-like regions)  
- Needs regularization to avoid fitting noise (same as classification trees)

---

## 10. Instability of Decision Trees

Decision Trees are **high-variance models**:

- Small changes in training data can lead to **very different trees**.  
- Early splits (near the root) are based on small advantages in cost, which can flip if the data changes slightly.  
- This makes single trees sensitive to noise and sample selection.

To combat instability:

- Use **simpler trees** (shallow depth, pruning).  
- Or use **ensembles**:
  - **Random Forests** ‚Äì many trees trained on bootstrapped samples + random feature subsets, averaged together.  
  - **Gradient Boosted Trees** ‚Äì trees added sequentially to correct errors of previous ones.

In practice:

- Single deep trees are rarely the final production model.  
- They are often used as:
  - **Baseline models**  
  - **Interpretable models** with shallow depth  
  - **Base learners** inside ensembles

---

## 11. Pros and Cons of Decision Trees

### 11.1 Advantages

- **Interpretability**
  - If the tree is not too deep, rules are easy to read and explain.
- **Handles mixed data types**
  - Works with numeric, categorical, and boolean features.
- **Non-linear modeling**
  - Captures non-linear relationships without feature scaling.
- **Little preprocessing needed**
  - No scaling/normalization required, robust to different units.
- **Fast prediction**
  - Time per prediction depends only on tree depth, which is typically small.
- **Great base learner**
  - Forms the backbone of many top-performing tabular models (Random Forest, XGBoost, LightGBM, etc.).

### 11.2 Disadvantages

- **Overfitting**
  - Deep trees can perfectly fit training data but generalize poorly.
- **High variance / instability**
  - Small changes in data ‚Üí very different tree structure.
- **Piecewise constant predictions**
  - For regression, predictions can be jagged and non-smooth.
- **Axis-aligned splits**
  - Needs many splits to approximate complex boundaries.
- **Not ideal alone for very high-dimensional sparse data**
  - Linear models or tree ensembles often perform better there.
